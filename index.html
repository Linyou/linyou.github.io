<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Youtian Lin</title>
  <meta name="author" content="Youtian Lin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center; font-family: 'Times New Roman', Times, serif;">
                    Youtian Lin æž—å°¤æ·»
                  </p>
                  <p>
                    I am a first-year Ph.D. student at <a href="https://www.nju.edu.cn/en/">Nanjing University</a>, supervised by <a href="https://yoyo000.github.io/index.html">Prof. Yao Yao</a> and <a href="https://is.nju.edu.cn/ttn_en/main.htm">Prof. Tieniu Tan</a>. My research focuses on 3D/4D reconstruction and generation. Previously, I pursued a Ph.D. at the <a href="https://www.hit.edu.cn">Harbin Institute of Technology</a>. I earned my M.S. from the <a href="https://english.hrbeu.edu.cn">Harbin Engineering University</a> in 2021, where I was advised by <a href="https://www.researchgate.net/profile/Jian-Guan-24">Prof. Jian Guan</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:linyoutian.loyot@gmail.com">Email</a> &nbsp;|&nbsp;
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=VhhHLhIAAAAJ">Scholar</a> &nbsp;|&nbsp;
                    <a href="https://twitter.com/linyoutian2">Twitter (X)</a> &nbsp;|&nbsp;
                    <a href="https://github.com/Linyou">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/head.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/head.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                    <p>
                      2024-09-27: Two papers accepted by NeurIPS 2024 ! ðŸŽ‰ðŸŽ‰ðŸŽ‰ <strong>Direct3D, FastDrag</strong>
                    </p>
                    <p>
                      2024-07-01: Three papers accepted by ECCV 2024 ! ðŸŽ‰ðŸŽ‰ðŸŽ‰ <strong>Relightable 3D Gaussian, STAG4D, UniDream</strong>
                    </p>
                    <p>
                      2024-05-24: New paper released! <strong>FastDrag</strong>
                    </p>
                    <p>
                      2024-05-23: New paper released! <strong>Direct3D</strong>
                    </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/fds.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nju-3dv.github.io/projects/fds/fds.pdf">
                    <span class="papertitle">Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors</span>
                  </a>
                  <br>
                    <a href="https://linzhuo.xyz">Lin-Zhuo Chen</a> *</sup>,
                    <a href="">Kangjie Liu</a> *</sup>,
                    <strong>Youtian Lin</strong>,
                    <a href="">Zhihao Li</a>,
                    <a href="https://siyuzhu-fudan.github.io/">Siyu Zhu</a>, 
                    <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
                    <a href="https://yoyo000.github.io/">Yao Yao</a>
                  <br>
                  <em>ICLR</em>, 2025 &nbsp
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/NJU-3DV/fds">Github</a>
                  |
                  <a href="https://nju-3dv.github.io/projects/fds/">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    A method for distilling geometric information from a pre-trained optical flow model into 3DGS. FDS samples unobserved views adjacent to the input views and calculates Prior-Flow to guide the analytically calculated Radiance-Flow.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/direct3d.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2405.14832">
                    <span class="papertitle">Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.it/citations?user=SN8J78EAAAAJ&hl=zh-CN">Shuang Wu*</a>,
                  <strong>Youtian Lin*</strong>,
                  <a href="http://www.feihuzhang.com">Feihu Zhang</a>,
                  <a href="https://github.com/zeng-yifei">Yifei Zeng</a>,
                  <a href="https://www.researchgate.net/profile/Jingxi-Xu-2">Jingxi Xu</a>,
                  <a href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ&hl=zh-CN">Philip Torr</a>, 
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>, 
                  <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
                  <br>
                  <em>NeurIPS</em>, 2024 &nbsp
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/DreamTechAI/Direct3D">Github</a>
                  |
                  <a href="https://nju-3dv.github.io/projects/Direct3D/">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    Direct3D introduces a scalable approach for generating high-quality 3D assets from images. It uses D3D-VAE for efficient 3D shape encoding and D3D-DiT for modeling 3D latents. This method setting a new standard for 3D content creation. 
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/fastdrag.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2405.15769">
                    <span class="papertitle">FastDrag: Manipulate Anything in One Step</span>
                  </a>
                  <br>
                  
                  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Xuanjia Zhao</a>,
                  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
                  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Congyi Fan</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=BfZECLsAAAAJ">Dongli Xu</a>,
                  <strong>Youtian Lin</strong>,
                  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Haiwei Pan</a>,
                  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>,
                  <br>
                  <em>NeurIPS</em>, 2024 &nbsp
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/XuanjiaZ/FastDrag">Github</a>
                  |
                  <a href="https://fastdrag-site.github.io">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    FastDrag is a new, faster drag-based image editing method that uses a latent warpage function for one-step pixel adjustment and a bilateral nearest neighbor interpolation to fill null regions. It also ensures consistency with the original image.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/stag4d.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.14939">
                    <span class="papertitle">STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians</span>
                  </a>
                  <br>
                  <a href="https://github.com/zeng-yifei">Yifei Zeng*</a>,
                  <a href="https://github.com/yanqinJiang">Yanqin Jiang*</a>,
                  <a href="https://sites.google.com/site/zhusiyucs/home/">Siyu Zhu</a>,
                  <a href="https://github.com/YuanxunLu">Yuanxun Lu</a>,
                  <strong>Youtian Lin</strong>,
                  <a href="http://zhuhao.cc/">Hao Zhu</a>,
                  <a href="https://people.ucas.ac.cn/~huweiming">Weiming Hu</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
                  <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
                  <br>
                  <em>ECCV</em>, 2024 &nbsp
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/zeng-yifei/STAG4D">Github</a> |
                  <a href="https://nju-3dv.github.io/projects/STAG4D/">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    High-fidelity 4D generation from diverse inputs (text, image, and video) with pre-trained diffusion models and dynamic 3D Gaussian splatting. 
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/gaussian-flow.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2312.03431">
                    <span class="papertitle">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</span>
                  </a>
                  <br>
                  <strong>Youtian Lin</strong>,
                  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=1W9jiEEAAAAJ">Zuozhuo Dai</a>,
                  <a href="https://sites.google.com/site/zhusiyucs/home">Siyu Zhu</a>,
                  <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
                  <br>
                  <em>CVPR</em>, 2024 (<font color="red">Highlight: 2.8%</font>) &nbsp
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/NJU-3DV/Gaussian-Flow">Github</a> |
                  <a href="https://nju-3dv.github.io/projects/Gaussian-Flow/">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    We propose an innovative point-based method for rapid dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos, leveraging advancements in point-based 3D Gaussian Splatting (3DGS). 
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/brdf-3dgs-2.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2311.16043">
                    <span class="papertitle">Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing</span>
                  </a>
                  <br>
                  <a href="https://ygaojiany.github.io">Jian Gao</a>,
                  <a href="https://sulvxiangxin.github.io">Chun Gu</a>,
                  <strong>Youtian Lin</strong>,
                  <a href="http://zhuhao.cc/home/">Hao Zhu</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
                  <a href="https://lzrobots.github.io">Li Zhang</a>,
                  <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
                  <br>
                  <em>ECCV</em>, 2024 &nbsp 
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/NJU-3DV/Relightable3DGaussian">Github</a> |
                  <a href="https://nju-3dv.github.io/projects/Relightable3DGaussian/">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    Utilizuing 3D Gaussian points to represent a scene, allowing for material and lighting decomposition, enabling real-time relighting, ray-tracing, and editing of the 3D point cloud with improved BRDF estimation and novel view rendering results. 
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/unidream-1.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yg256li.github.io/UniDream/">
                    <span class="papertitle">UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=bkPJckwAAAAJ">Zexiang Liu*</a>,
                  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN">Yangguang Li*</a>,
                  <strong>Youtian Lin*</strong>,
                  <a href="https://scholar.google.com/citations?user=JX8kSoEAAAAJ&hl=zh-CN">Xin Yu</a>,
                  <a href="https://pengsida.net">Sida Peng</a>,
                  <a href="https://yanpei.me">Yan-Pei Cao</a>,
                  <a href="https://xjqi.github.io">Xiaojuan Qi</a>,
                  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Xiaoshui Huang</a>,
                  <a href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ&hl=zh-CN">Ding Liang</a>,
                  <a href="https://wlouyang.github.io">Wanli Ouyang</a>
                  <br>
                  <em>ECCV</em>, 2024 &nbsp 
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/YG256Li/UniDream">Github</a> |
                  <a href="https://yg256li.github.io/UniDream/">Project Page</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    Use a dual-phase training process for albedo-normal aligned multi-view diffusion and reconstruction models, a progressive generation procedure for geometry and albedo-textures using Score Distillation Sample (SDS), and an innovative SDS application for finalizing Physically Based Rendering (PBR) generation with fixed albedo. 
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/ced-nerf2.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/Linyou/Ced-NeRF">
                    <span class="papertitle">Ced-NeRF: A Compact and Efficient Method for Dynamic Neural Radiance Fields</span>
                  </a>
                  <br>
                  <strong>Youtian Lin</strong>
                  <br>
                  <em>AAAI</em>, 2024 &nbsp 
                  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                  <br>
                  <a href="https://github.com/Linyou/Ced-NeRF">Github</a>
                  <!-- /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                  <p></p>
                  <p>
                    We extend the Instant-NGP framework to support dynamic scenes, and show that it can be used to train a dynamic NeRF model that is both more compact and more efficient than prior work. 
                  </p>
                </td>
              </tr>

              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <h2> Previous Works </h2>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">

              </td>
              </tr>

              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/earl.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10238752">
                  <span class="papertitle">EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images</span>
                </a>
                <br>
                <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
                <a href="https://justlovesmile.github.io/">Mingjie Xie</a>,
                <strong>Youtian Lin</strong>,
                <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN">Guangjun He</a>,
                <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>
                <br>
                <em>IEEE TGRS</em>, 2023 &nbsp 
                <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <a href="https://github.com/Justlovesmile/EARL">Github</a>
                <p></p>
                <p>
                  Incorporating adaptive scale sampling, dynamic elliptical distribution aided sampling, and spatial distance weighting to enhance the selection of high-quality positive samples.
                </p>
              </td>
              </tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/toso.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9053562">
                  <span class="papertitle">TOSO: Student's-T Distribution Aided One-Stage Orientation Target Detection in Remote Sensing Images</span>
                </a>
                <br>
                <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng*</a>,
                <strong>Youtian Lin*</strong>,
                <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
                <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN"> Guangjun He</a>,
                <a href="https://linyou.github.io/youtian-page/">Huifeng Shi</a>,
                <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=UeOcj28AAAAJ">Jonathon Chambers</a>
                <br>
                <em>ICASSP</em>, 2020 &nbsp 
                <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <!-- <a href="https://github.com/Linyou/Ced-NeRF">Project Page</a> -->
                <!-- /
                <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                /
                <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
                <p></p>
                <p>
                  Utilizing a one-stage keypoint based network architecture and introducing a novel geometric transformation method to achieve orientation angle regression, along with incorporating Student's-t distribution to enhance performance
                </p>
              </td>
              </tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ienet.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1912.00969">
                  <span class="papertitle">IENet: Interacting Embranchment One Stage Anchor Free Detector for Orientation Aerial Object Detection</span>
                </a>
                <br>
                <strong>Youtian Lin</strong>,
                <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>,
                <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
                <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=JQFnV5IAAAAJ">Wenwu Wang</a>,
                <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=UeOcj28AAAAJ">Jonathon Chambers</a>
                <br>
                <em>Arxiv</em>, 2019 &nbsp 
                <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
                <br>
                <p></p>
                <p>
                  We addressing the challenges of computational complexity in two-stage detectors by employing a per-pixel prediction approach with a geometric transformation, a branch interactive module, and an enhanced intersection over union (IoU) loss. 
                </p>
              </td>
              </tr>

            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Project</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/github-mark.png" alt="piano" width="120"></td>
              <td width="75%" valign="center">
                <p>
                  <a href="https://github.com/DSaurus/threestudio-3dgs">
                    <papertitle>pointrix-project</papertitle> 
                  </a><br>
                  Pointrix: a differentiable point-based rendering library. 
                  <br><br>
                  <a href="https://github.com/DSaurus/threestudio-3dgs">
                    <papertitle>threestudio-3dgs</papertitle> 
                  </a><br>
                  The Gaussian Splatting extension for threestudio. 
                  <br><br>
                  <a href="https://github.com/taichi-dev/taichi-nerfs">
                    <papertitle>taichi-nerfs</papertitle>
                  </a>
                  <br>
                  A PyTorch + Taichi implementation of instant-ngp NeRF training pipeline. For more details about modeling, please checkout <a href="https://docs.taichi-lang.org/blog/taichi-instant-ngp">this blog</a>.
                  <br><br>
                  <a href="https://github.com/Linyou/taichi-ngp-renderer">
                    <papertitle>taichi-ngp-renderer</papertitle> 
                  </a><br>
                  A Instant-NGP renderer implemented using Taichi, written entirely in Python. No CUDA! 
                  <br><br>
                  <a href="https://github.com/nerfstudio-project/nerfacc">
                    <papertitle>nerfacc</papertitle> 
                  </a><br>
                  A PyTorch Nerf acceleration toolbox for both training and inference. (<strong>As a contributor, I have implemented a fast ngp rendering in CUDA</strong>) 
                  <br><br>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code borrow from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </tbody>
  </table>
</body>
</html>
